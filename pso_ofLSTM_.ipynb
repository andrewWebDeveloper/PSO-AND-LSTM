{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pPycT5RwX00N"
      },
      "outputs": [],
      "source": [
        "#importing the necessary libraries and dependencies\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import seaborn as sns\n",
        "from sklearn import preprocessing\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "import matplotlib.pyplot as plt\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from numpy import array\n",
        "from keras.models import Sequential\n",
        "from keras.layers import LSTM\n",
        "from keras.layers import Dense\n",
        "from keras import optimizers\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from sklearn.metrics import mean_squared_error"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "akvV9LoX6Pjh",
        "outputId": "51ffedd8-ebf5-4410-d4b0-3cbbf5b6d8b7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d-goh4G4X16J",
        "outputId": "78c569d8-16eb-4991-f5b8-67a01c1bd325"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-4-11471905d095>:4: DtypeWarning: Columns (15) have mixed types. Specify dtype option on import or set low_memory=False.\n",
            "  df3=pd.read_csv(\"/content/drive/MyDrive/7048/station_hour.csv\")\n"
          ]
        }
      ],
      "source": [
        "df=pd.read_csv(\"/content/drive/MyDrive/7048/city_day.csv\")\n",
        "df1=pd.read_csv(\"/content/drive/MyDrive/7048/city_hour.csv\")\n",
        "df2=pd.read_csv(\"/content/drive/MyDrive/7048/station_day.csv\")\n",
        "df3=pd.read_csv(\"/content/drive/MyDrive/7048/station_hour.csv\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6KRYfwvbWNSY",
        "outputId": "d1948609-912b-4350-ac00-b8a9d772b179"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-5-f9ad4cc37a63>:1: FutureWarning: Dropping of nuisance columns in DataFrame reductions (with 'numeric_only=None') is deprecated; in a future version this will raise TypeError.  Select only valid columns before calling the reduction.\n",
            "  df.fillna(df.mean(), inplace=True)\n",
            "<ipython-input-5-f9ad4cc37a63>:2: FutureWarning: Dropping of nuisance columns in DataFrame reductions (with 'numeric_only=None') is deprecated; in a future version this will raise TypeError.  Select only valid columns before calling the reduction.\n",
            "  df1.fillna(df.mean(), inplace=True)\n",
            "<ipython-input-5-f9ad4cc37a63>:3: FutureWarning: Dropping of nuisance columns in DataFrame reductions (with 'numeric_only=None') is deprecated; in a future version this will raise TypeError.  Select only valid columns before calling the reduction.\n",
            "  df2.fillna(df.mean(), inplace=True)\n",
            "<ipython-input-5-f9ad4cc37a63>:4: FutureWarning: Dropping of nuisance columns in DataFrame reductions (with 'numeric_only=None') is deprecated; in a future version this will raise TypeError.  Select only valid columns before calling the reduction.\n",
            "  df2.fillna(df.mean(), inplace=True)\n"
          ]
        }
      ],
      "source": [
        "df.fillna(df.mean(), inplace=True)\n",
        "df1.fillna(df.mean(), inplace=True)\n",
        "df2.fillna(df.mean(), inplace=True)\n",
        "df2.fillna(df.mean(), inplace=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "qwn1xePLmlTl",
        "outputId": "2db70e15-ac64-4240-ca20-ec8ff021a3c6"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "  StationId             Datetime  PM2.5    PM10    NO    NO2    NOx    NH3  \\\n",
              "0     AP001  2017-11-24 17:00:00  60.50   98.00  2.35  30.80  18.25   8.50   \n",
              "1     AP001  2017-11-24 18:00:00  65.50  111.25  2.70  24.20  15.07   9.77   \n",
              "2     AP001  2017-11-24 19:00:00  80.00  132.00  2.10  25.18  15.15  12.02   \n",
              "3     AP001  2017-11-24 20:00:00  81.50  133.25  1.95  16.25  10.23  11.58   \n",
              "4     AP001  2017-11-24 21:00:00  75.25  116.00  1.43  17.48  10.43  12.03   \n",
              "\n",
              "    CO    SO2      O3  Benzene  Toluene  Xylene  AQI AQI_Bucket  \n",
              "0  0.1  11.85  126.40      0.1     6.10    0.10  NaN        NaN  \n",
              "1  0.1  13.17  117.12      0.1     6.25    0.15  NaN        NaN  \n",
              "2  0.1  12.08   98.98      0.2     5.98    0.18  NaN        NaN  \n",
              "3  0.1  10.47  112.20      0.2     6.72    0.10  NaN        NaN  \n",
              "4  0.1   9.12  106.35      0.2     5.75    0.08  NaN        NaN  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-5fcc3016-26d5-4c76-8b30-d281554c03dd\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>StationId</th>\n",
              "      <th>Datetime</th>\n",
              "      <th>PM2.5</th>\n",
              "      <th>PM10</th>\n",
              "      <th>NO</th>\n",
              "      <th>NO2</th>\n",
              "      <th>NOx</th>\n",
              "      <th>NH3</th>\n",
              "      <th>CO</th>\n",
              "      <th>SO2</th>\n",
              "      <th>O3</th>\n",
              "      <th>Benzene</th>\n",
              "      <th>Toluene</th>\n",
              "      <th>Xylene</th>\n",
              "      <th>AQI</th>\n",
              "      <th>AQI_Bucket</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>AP001</td>\n",
              "      <td>2017-11-24 17:00:00</td>\n",
              "      <td>60.50</td>\n",
              "      <td>98.00</td>\n",
              "      <td>2.35</td>\n",
              "      <td>30.80</td>\n",
              "      <td>18.25</td>\n",
              "      <td>8.50</td>\n",
              "      <td>0.1</td>\n",
              "      <td>11.85</td>\n",
              "      <td>126.40</td>\n",
              "      <td>0.1</td>\n",
              "      <td>6.10</td>\n",
              "      <td>0.10</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>AP001</td>\n",
              "      <td>2017-11-24 18:00:00</td>\n",
              "      <td>65.50</td>\n",
              "      <td>111.25</td>\n",
              "      <td>2.70</td>\n",
              "      <td>24.20</td>\n",
              "      <td>15.07</td>\n",
              "      <td>9.77</td>\n",
              "      <td>0.1</td>\n",
              "      <td>13.17</td>\n",
              "      <td>117.12</td>\n",
              "      <td>0.1</td>\n",
              "      <td>6.25</td>\n",
              "      <td>0.15</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>AP001</td>\n",
              "      <td>2017-11-24 19:00:00</td>\n",
              "      <td>80.00</td>\n",
              "      <td>132.00</td>\n",
              "      <td>2.10</td>\n",
              "      <td>25.18</td>\n",
              "      <td>15.15</td>\n",
              "      <td>12.02</td>\n",
              "      <td>0.1</td>\n",
              "      <td>12.08</td>\n",
              "      <td>98.98</td>\n",
              "      <td>0.2</td>\n",
              "      <td>5.98</td>\n",
              "      <td>0.18</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>AP001</td>\n",
              "      <td>2017-11-24 20:00:00</td>\n",
              "      <td>81.50</td>\n",
              "      <td>133.25</td>\n",
              "      <td>1.95</td>\n",
              "      <td>16.25</td>\n",
              "      <td>10.23</td>\n",
              "      <td>11.58</td>\n",
              "      <td>0.1</td>\n",
              "      <td>10.47</td>\n",
              "      <td>112.20</td>\n",
              "      <td>0.2</td>\n",
              "      <td>6.72</td>\n",
              "      <td>0.10</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>AP001</td>\n",
              "      <td>2017-11-24 21:00:00</td>\n",
              "      <td>75.25</td>\n",
              "      <td>116.00</td>\n",
              "      <td>1.43</td>\n",
              "      <td>17.48</td>\n",
              "      <td>10.43</td>\n",
              "      <td>12.03</td>\n",
              "      <td>0.1</td>\n",
              "      <td>9.12</td>\n",
              "      <td>106.35</td>\n",
              "      <td>0.2</td>\n",
              "      <td>5.75</td>\n",
              "      <td>0.08</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-5fcc3016-26d5-4c76-8b30-d281554c03dd')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-5fcc3016-26d5-4c76-8b30-d281554c03dd button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-5fcc3016-26d5-4c76-8b30-d281554c03dd');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ],
      "source": [
        "df3.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PCB4LMLlYvu8",
        "outputId": "330cec48-d148-4db6-ebdf-0838f661cd30"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "City             0\n",
              "Date             0\n",
              "PM2.5            0\n",
              "PM10             0\n",
              "NO               0\n",
              "NO2              0\n",
              "NOx              0\n",
              "NH3              0\n",
              "CO               0\n",
              "SO2              0\n",
              "O3               0\n",
              "Benzene          0\n",
              "Toluene          0\n",
              "Xylene           0\n",
              "AQI              0\n",
              "AQI_Bucket    4681\n",
              "dtype: int64"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ],
      "source": [
        "df.isnull().sum()\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9rdoNeGebn_L"
      },
      "outputs": [],
      "source": [
        "df['PM2.5'].fillna(method = 'bfill',inplace = True )\n",
        "df['PM10'].fillna(method = 'bfill',inplace = True )\n",
        "df['NO'].fillna(method = 'ffill',inplace = True )\n",
        "df['NO2'].fillna(method = 'ffill',inplace = True )\n",
        "df['NOx'].fillna(method = 'ffill',inplace = True )\n",
        "df['NH3'].fillna(method = 'bfill',inplace = True )\n",
        "df['CO'].fillna(method = 'ffill',inplace = True )\n",
        "df['SO2'].fillna(method = 'ffill',inplace = True )\n",
        "df['O3'].fillna(method = 'ffill',inplace = True )\n",
        "df['Benzene'].fillna(method = 'ffill',inplace = True )\n",
        "df['Toluene'].fillna(method = 'ffill',inplace = True )\n",
        "df['Xylene'].fillna(method = 'ffill',inplace = True )\n",
        "df['AQI'].fillna(method = 'bfill',inplace = True )\n",
        "df['AQI_Bucket'].fillna(method = 'bfill',inplace = True )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1l9_zUx_7YjS"
      },
      "outputs": [],
      "source": [
        "df1['PM2.5'].fillna(method = 'bfill',inplace = True )\n",
        "df1['PM10'].fillna(method = 'bfill',inplace = True )\n",
        "df1['NO'].fillna(method = 'ffill',inplace = True )\n",
        "df1['NO2'].fillna(method = 'ffill',inplace = True )\n",
        "df1['NOx'].fillna(method = 'ffill',inplace = True )\n",
        "df1['NH3'].fillna(method = 'bfill',inplace = True )\n",
        "df1['CO'].fillna(method = 'ffill',inplace = True )\n",
        "df1['SO2'].fillna(method = 'ffill',inplace = True )\n",
        "df1['O3'].fillna(method = 'ffill',inplace = True )\n",
        "df1['Benzene'].fillna(method = 'ffill',inplace = True )\n",
        "df1['Toluene'].fillna(method = 'ffill',inplace = True )\n",
        "df1['Xylene'].fillna(method = 'ffill',inplace = True )\n",
        "df1['AQI'].fillna(method = 'bfill',inplace = True )\n",
        "df1['AQI_Bucket'].fillna(method = 'bfill',inplace = True )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FHuzg-KB7Zgc"
      },
      "outputs": [],
      "source": [
        "df2['PM2.5'].fillna(method = 'bfill',inplace = True )\n",
        "df2['PM10'].fillna(method = 'bfill',inplace = True )\n",
        "df2['NO'].fillna(method = 'ffill',inplace = True )\n",
        "df2['NO2'].fillna(method = 'ffill',inplace = True )\n",
        "df2['NOx'].fillna(method = 'ffill',inplace = True )\n",
        "df2['NH3'].fillna(method = 'bfill',inplace = True )\n",
        "df2['CO'].fillna(method = 'ffill',inplace = True )\n",
        "df2['SO2'].fillna(method = 'ffill',inplace = True )\n",
        "df2['O3'].fillna(method = 'ffill',inplace = True )\n",
        "df2['Benzene'].fillna(method = 'ffill',inplace = True )\n",
        "df2['Toluene'].fillna(method = 'ffill',inplace = True )\n",
        "df2['Xylene'].fillna(method = 'ffill',inplace = True )\n",
        "df2['AQI'].fillna(method = 'bfill',inplace = True )\n",
        "df2['AQI_Bucket'].fillna(method = 'bfill',inplace = True )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vVFYps0aqqqe"
      },
      "outputs": [],
      "source": [
        "df3['PM2.5'].fillna(method = 'bfill',inplace = True )\n",
        "df3['PM10'].fillna(method = 'bfill',inplace = True )\n",
        "df3['NO'].fillna(method = 'ffill',inplace = True )\n",
        "df3['NO2'].fillna(method = 'ffill',inplace = True )\n",
        "df3['NOx'].fillna(method = 'ffill',inplace = True )\n",
        "df3['NH3'].fillna(method = 'bfill',inplace = True )\n",
        "df3['CO'].fillna(method = 'ffill',inplace = True )\n",
        "df3['SO2'].fillna(method = 'ffill',inplace = True )\n",
        "df3['O3'].fillna(method = 'ffill',inplace = True )\n",
        "df3['Benzene'].fillna(method = 'ffill',inplace = True )\n",
        "df3['Toluene'].fillna(method = 'ffill',inplace = True )\n",
        "df3['Xylene'].fillna(method = 'ffill',inplace = True )\n",
        "df3['AQI'].fillna(method = 'bfill',inplace = True )\n",
        "df3['AQI_Bucket'].fillna(method = 'bfill',inplace = True )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QB-uV2E4boFe",
        "outputId": "a67d368a-a82c-4ca3-fdb3-92442b58f920"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "City          0\n",
              "Date          0\n",
              "PM2.5         0\n",
              "PM10          0\n",
              "NO            0\n",
              "NO2           0\n",
              "NOx           0\n",
              "NH3           0\n",
              "CO            0\n",
              "SO2           0\n",
              "O3            0\n",
              "Benzene       0\n",
              "Toluene       0\n",
              "Xylene        0\n",
              "AQI           0\n",
              "AQI_Bucket    0\n",
              "dtype: int64"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ],
      "source": [
        "df.isnull().sum()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OQyEmOjx4fdo"
      },
      "outputs": [],
      "source": [
        "def city_wise_pollution_based(pollutant,color):\n",
        "    a=df[[pollutant, 'City']].groupby(['City']).median().sort_values(pollutant, ascending = False)\n",
        "    a.plot(color=color,figsize=(12,4))\n",
        "    plt.legend(fontsize=20)\n",
        "    plt.xticks(fontsize=15)\n",
        "    plt.title(\"AMOUNT OF CONCENTRATION OF POLLUTANTS - CITY BASED \"+\"( \" + pollutant + \" )\")\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FmvI1QVR5jlQ"
      },
      "source": [
        "evry features are importent\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 91
        },
        "id": "SYTZHh3W4bbA",
        "outputId": "948ca5c6-a067-4d8e-8af7-320c15ddc859"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'city_wise_pollution_based(\\'PM2.5\\',\\'#CD6155\\')\\ncity_wise_pollution_based(\\'PM10\\',\\'teal\\')\\ncity_wise_pollution_based(\\'NO2\\',\\'#2980B9\\')\\ncity_wise_pollution_based(\\'NH3\\',\\'#B9770E\\')\\ncity_wise_pollution_based(\\'CO\\',\\'#283747\\')\\ncity_wise_pollution_based(\\'SO2\\',\\'#99004d\\')\\ncity_wise_pollution_based(\\'O3\\',\\'#002db3\\')\\ncity_wise_pollution_based(\\'Benzene\\',\\'#00b38f\\')\\ncity_wise_pollution_based(\\'Toluene\\',\\'#1f7a1f\\')\\ncity_wise_pollution_based(\\'Xylene\\',\\'#662200\\')\\nprint(\"plots\")'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 14
        }
      ],
      "source": [
        "\"\"\"city_wise_pollution_based('PM2.5','#CD6155')\n",
        "city_wise_pollution_based('PM10','teal')\n",
        "city_wise_pollution_based('NO2','#2980B9')\n",
        "city_wise_pollution_based('NH3','#B9770E')\n",
        "city_wise_pollution_based('CO','#283747')\n",
        "city_wise_pollution_based('SO2','#99004d')\n",
        "city_wise_pollution_based('O3','#002db3')\n",
        "city_wise_pollution_based('Benzene','#00b38f')\n",
        "city_wise_pollution_based('Toluene','#1f7a1f')\n",
        "city_wise_pollution_based('Xylene','#662200')\n",
        "print(\"plots\")\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Zvf32C4Z7w__"
      },
      "outputs": [],
      "source": [
        "#df['Date'] = pd.DatetimeIndex(df['Date']).year\n",
        "#df['month'] = pd.DatetimeIndex(df['Date']).month\n",
        "#df['day'] = pd.DatetimeIndex(df['Date']).day"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jouRn0PfboSh"
      },
      "outputs": [],
      "source": [
        "df[\"AQI_Bucket\"]=df[\"AQI_Bucket\"].replace({'Poor':4, 'Very Poor':5, 'Severe':6,'Moderate':3,'Satisfactory':2, 'Good':1})\n",
        "df1[\"AQI_Bucket\"]=df1[\"AQI_Bucket\"].replace({'Poor':4, 'Very Poor':5, 'Severe':6,'Moderate':3,'Satisfactory':2, 'Good':1})\n",
        "df2[\"AQI_Bucket\"]=df2[\"AQI_Bucket\"].replace({'Poor':4, 'Very Poor':5, 'Severe':6,'Moderate':3,'Satisfactory':2, 'Good':1})\n",
        "df3[\"AQI_Bucket\"]=df2[\"AQI_Bucket\"].replace({'Poor':4, 'Very Poor':5, 'Severe':6,'Moderate':3,'Satisfactory':2, 'Good':1})"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xbKLGQ44u69-"
      },
      "outputs": [],
      "source": [
        "\n",
        "x=df.drop(columns=[\"Date\",\"City\",'AQI_Bucket'],axis=1)\n",
        "y = df['AQI_Bucket']\n",
        "y = y.to_frame()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qzY9yUv_xP60",
        "outputId": "693a4fde-cd3c-4e5e-9f54-7c4aed9c401c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 29531 entries, 0 to 29530\n",
            "Data columns (total 1 columns):\n",
            " #   Column      Non-Null Count  Dtype\n",
            "---  ------      --------------  -----\n",
            " 0   AQI_Bucket  29531 non-null  int64\n",
            "dtypes: int64(1)\n",
            "memory usage: 230.8 KB\n"
          ]
        }
      ],
      "source": [
        "y.info()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-JnQcvGN6LrP"
      },
      "outputs": [],
      "source": [
        "x1=df1.drop(columns=[\"Datetime\",\"City\",'AQI_Bucket'],axis=1)\n",
        "y1=df1['AQI_Bucket']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8bpeEfGT6Rng"
      },
      "outputs": [],
      "source": [
        "x2=df2.drop(columns=[\"Date\",\"StationId\",'AQI_Bucket'],axis=1)\n",
        "y2=df2['AQI_Bucket']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ELO8WBuMrFUP"
      },
      "outputs": [],
      "source": [
        "x3=df3.drop(columns=[\"Datetime\",\"StationId\",'AQI_Bucket'],axis=1)\n",
        "y3=df3['AQI_Bucket']"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pH1wi0O-3AGa"
      },
      "source": [
        "this one"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9-s1FQhBx4_7"
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "from keras.optimizers import SGD\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "plt.style.use('fivethirtyeight')\n",
        "import pandas as pd\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, LSTM, Dropout, GRU, Bidirectional\n",
        "from keras.optimizers import SGD\n",
        "import math\n",
        "from sklearn.metrics import mean_squared_error,r2_score,mean_absolute_error\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lrPE4xWr6QCX"
      },
      "outputs": [],
      "source": [
        "def mape(actual, predicted):\n",
        "    return np.mean(np.abs((actual - predicted) / actual)) * 100\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ys-8Bvt0yVh1",
        "outputId": "42dff19e-f07a-4c99-8f58-457fa1c48b9c"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "((20671, 1), (20671, 13), (8860, 1), (8860, 13))"
            ]
          },
          "metadata": {},
          "execution_count": 24
        }
      ],
      "source": [
        "X_train, X_test, y_train, y_test = train_test_split(x, y, test_size=0.30, random_state=11)\n",
        "y_train.shape,X_train.shape,y_test.shape,X_test.shape\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3sMYABtw8n61",
        "outputId": "e3783923-53b1-4878-9d72-1ab690c38897"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "((495512,), (495512, 13), (212363,), (212363, 13))"
            ]
          },
          "metadata": {},
          "execution_count": 25
        }
      ],
      "source": [
        "X_train1, X_test1, y_train1, y_test1 = train_test_split(x1, y1, test_size=0.30, random_state=11)\n",
        "y_train1.shape,X_train1.shape,y_test1.shape,X_test1.shape\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gqvpC5of8oJO",
        "outputId": "161f1f57-a893-4345-8b58-d789f190176e"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "((75624,), (75624, 13), (32411,), (32411, 13))"
            ]
          },
          "metadata": {},
          "execution_count": 26
        }
      ],
      "source": [
        "X_train2, X_test2, y_train2, y_test2 = train_test_split(x2, y2, test_size=0.30, random_state=11)\n",
        "y_train2.shape,X_train2.shape,y_test2.shape,X_test2.shape\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Dru9rdqKrPVG",
        "outputId": "fe1f9f99-3c9e-4010-c476-654fdf41d3a7"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "((1812358,), (1812358, 13), (776725,), (776725, 13))"
            ]
          },
          "metadata": {},
          "execution_count": 27
        }
      ],
      "source": [
        "X_train3, X_test3, y_train3, y_test3 = train_test_split(x3, y3, test_size=0.30, random_state=11)\n",
        "y_train3.shape,X_train3.shape,y_test3.shape,X_test3.shape\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "a0S2eD5qgCb-"
      },
      "outputs": [],
      "source": [
        "#!pip install pyswarms"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tN-hYCklk_a-"
      },
      "source": [
        "**DATASET 1**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 153
        },
        "id": "e_WZvIVbfyRt",
        "outputId": "7c7d6db3-385d-46b1-f7c0-566a7b658860"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "SyntaxError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-29-d70835a176bc>\"\u001b[0;36m, line \u001b[0;32m129\u001b[0m\n\u001b[0;31m    print(\"Optimal Validation Loss: \", optimal_val_loss)\u001b[0m\n\u001b[0m                                                        \n^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m EOF while scanning triple-quoted string literal\n"
          ]
        }
      ],
      "source": [
        "\"\"\"import numpy as np\n",
        "import random\n",
        "from sklearn.model_selection import train_test_split\n",
        "from keras.models import Sequential\n",
        "from keras.layers import GRU\n",
        "from keras.optimizers import Adam\n",
        "from keras.callbacks import EarlyStopping\n",
        "\n",
        "class Particle:\n",
        "    def __init__(self, x0):\n",
        "        self.position = x0\n",
        "        self.velocity = np.array([np.random.uniform(-1,1) for i in range(len(x0))])\n",
        "        self.best_pos_in = x0\n",
        "        self.best_pos_in\n",
        "        self.best_cost_in = float(\"-inf\")\n",
        "        \n",
        "\n",
        "    def update_velocity(self, global_best, c1=2, c2=2, w=0.7):\n",
        "        r1 = np.random.rand(len(self.velocity))\n",
        "        r2 = np.random.rand(len(self.velocity))\n",
        "\n",
        "        cognitive_tip = np.array(self.best_pos_in) - np.array(self.position)\n",
        "        cognitive_component = c1 * r1 * cognitive_tip\n",
        "\n",
        "        social_component = c2*r2*(global_best - self.position)\n",
        "        self.velocity = w*self.velocity + cognitive_component + social_component\n",
        "\n",
        "    def update_position(self, bounds, dim):\n",
        "        self.position += self.velocity\n",
        "        for i in range(dim):\n",
        "            if self.position[i] > bounds[i][1]:\n",
        "                self.position[i] = bounds[i][1]\n",
        "            elif self.position[i] < bounds[i][0]:\n",
        "                self.position[i] = bounds[i][0]\n",
        "\n",
        "class PSO:\n",
        "    def __init__(self, costFunc, x0, bounds, num_particles, maxiter):\n",
        "        self.costFunc = costFunc\n",
        "        self.num_particles = num_particles\n",
        "        self.dim = len(x0)\n",
        "        self.bounds = bounds\n",
        "        self.particles = []\n",
        "        self.gbest = np.zeros(self.dim)\n",
        "        self.gbest_cost = float(\"-inf\")\n",
        "        self.maxiter = maxiter\n",
        "\n",
        "        for i in range(self.num_particles):\n",
        "            self.particles.append(Particle(x0))\n",
        "\n",
        "\n",
        "    def run(self):\n",
        "        for i in range(self.maxiter):\n",
        "            for particle in self.particles:  \n",
        "        \n",
        "                cost = self.costFunc(particle.position)\n",
        "                \n",
        "                  \n",
        "                if cost < particle.best_cost_in:\n",
        "                    particle.best_cost_in = cost\n",
        "\n",
        "                    particle.best_pos_in = particle.position\n",
        "\n",
        "                if cost < self.gbest_cost:\n",
        "                    self.gbest_cost = cost\n",
        "                    self.gbest = particle.position\n",
        "\n",
        "            for particle in self.particles:\n",
        "                particle.update_velocity(self.gbest)\n",
        "                particle.update_position(self.bounds, self.dim)\n",
        "            break\n",
        "        return self.gbest, self.gbest_cost\n",
        "        \n",
        "\n",
        "# Define cost function for training the model\n",
        "\n",
        "\n",
        "def cost_function(position):\n",
        "  \n",
        "    # Define the LSTM-RNN model\n",
        "    model = Sequential()\n",
        "    model.add(LSTM(position[0], input_shape=(X_train.shape[1],1), return_sequences=True))\n",
        "    model.add(LSTM(position[1]))\n",
        "    model.add(Dense(y_train.shape[1], activation='linear'))\n",
        "    \n",
        "    model.compile(optimizer=SGD(learning_rate=0.01, decay=1e-6, momentum=0.9, nesterov=False),\n",
        "                  loss='mean_squared_error', metrics=['mae'])\n",
        "\n",
        "    # Fit the model to the training set\n",
        "    history = model.fit(X_train, y_train, epochs=5, batch_size=32, validation_data=(X_test, y_test))\n",
        "\n",
        "    # Make predictions on test data\n",
        "    y_pred_train = model.predict(X_train)\n",
        "    y_pred_test = model.predict(X_test)\n",
        "\n",
        "    # Evaluate model performance on training set\n",
        "    r2_train = r2_score(y_train, y_pred_train)\n",
        "    print('R2 score on training set: {:.2f}'.format(r2_train))\n",
        "    mse_train = mean_squared_error(y_train, y_pred_train)\n",
        "    print(f\"Mean squared error on the training set: {mse_train:.3f}\")\n",
        "    map_train = mean_absolute_error(y_train, y_pred_train)\n",
        "    print(f'Mean Absolute Percentage Error on training set:{map_train:.3f}')\n",
        "    rmse_train = np.sqrt(mse_train)\n",
        "    print(f\"Root Mean squared error on the training set: {rmse_train:.3f}\")\n",
        "\n",
        "    # Evaluate model performance on test set\n",
        "    r2_test = r2_score(y_test, y_pred_test)\n",
        "    print('R2 score on test set: {:.2f}'.format(r2_test))\n",
        "    mse_test = mean_squared_error(y_test, y_pred_test)\n",
        "    print(f\"Mean squared error on the test set: {mse_test:.3f}\")\n",
        "    map_test = mean_absolute_error(y_test, y_pred_test)\n",
        "    print(f'Mean Absolute Percentage Error on test set:{map_test:.3f}')\n",
        "    rmse_test = np.sqrt(mse_test)\n",
        "    print(f\"Root Mean squared error on the test set: {rmse_test:.3f}\")\n",
        "\n",
        "    val_loss = min(history.history['val_loss'])\n",
        "    return val_loss \n",
        "\n",
        "# Define the initial position, bounds, and other parameters for PSO\n",
        "x0 = [16,16]  # Initial hidden size for both GRU layers\n",
        "bounds = [ (8, 32), (8, 32)] #  constraints on hidden size\n",
        "num_particles = 10 # number of particles\n",
        "maxiter = 20\n",
        "hidden_size = 64\n",
        "\n",
        "# Create an instance of the PSO class and run the optimization\n",
        "pso = PSO(cost_function, x0, bounds, num_particles, maxiter)\n",
        "optimal_hidden_sizes, optimal_val_loss = pso.run()\n",
        "print(\"Optimal Hidden Sizes: \", optimal_hidden_sizes)\n",
        "print(\"Optimal Validation Loss: \", optimal_val_loss)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5KRQjjBnmItf"
      },
      "source": [
        "**DATASET 2**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KkI67FlWxmtE"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import random\n",
        "from sklearn.model_selection import train_test_split\n",
        "from keras.models import Sequential\n",
        "from keras.layers import GRU\n",
        "from keras.optimizers import Adam\n",
        "from keras.callbacks import EarlyStopping\n",
        "\n",
        "class Particle:\n",
        "    def __init__(self, x0):\n",
        "        self.position = x0\n",
        "        self.velocity = np.array([np.random.uniform(-1,1) for i in range(len(x0))])\n",
        "        self.best_pos_in = x0\n",
        "        self.best_pos_in\n",
        "        self.best_cost_in = float(\"-inf\")\n",
        "        \n",
        "\n",
        "    def update_velocity(self, global_best, c1=2, c2=2, w=0.7):\n",
        "        r1 = np.random.rand(len(self.velocity))\n",
        "        r2 = np.random.rand(len(self.velocity))\n",
        "\n",
        "        cognitive_tip = np.array(self.best_pos_in) - np.array(self.position)\n",
        "        cognitive_component = c1 * r1 * cognitive_tip\n",
        "\n",
        "        social_component = c2*r2*(global_best - self.position)\n",
        "        self.velocity = w*self.velocity + cognitive_component + social_component\n",
        "\n",
        "    def update_position(self, bounds, dim):\n",
        "        self.position += self.velocity\n",
        "        for i in range(dim):\n",
        "            if self.position[i] > bounds[i][1]:\n",
        "                self.position[i] = bounds[i][1]\n",
        "            elif self.position[i] < bounds[i][0]:\n",
        "                self.position[i] = bounds[i][0]\n",
        "\n",
        "class PSO:\n",
        "    def __init__(self, costFunc, x0, bounds, num_particles, maxiter):\n",
        "        self.costFunc = costFunc\n",
        "        self.num_particles = num_particles\n",
        "        self.dim = len(x0)\n",
        "        self.bounds = bounds\n",
        "        self.particles = []\n",
        "        self.gbest = np.zeros(self.dim)\n",
        "        self.gbest_cost = float(\"-inf\")\n",
        "        self.maxiter = maxiter\n",
        "\n",
        "        for i in range(self.num_particles):\n",
        "            self.particles.append(Particle(x0))\n",
        "\n",
        "\n",
        "    def run(self):\n",
        "        for i in range(self.maxiter):\n",
        "            for particle in self.particles:  \n",
        "        \n",
        "                cost = self.costFunc(particle.position)\n",
        "                \n",
        "                  \n",
        "                if cost < particle.best_cost_in:\n",
        "                    particle.best_cost_in = cost\n",
        "\n",
        "                    particle.best_pos_in = particle.position\n",
        "\n",
        "                if cost < self.gbest_cost:\n",
        "                    self.gbest_cost = cost\n",
        "                    self.gbest = particle.position\n",
        "\n",
        "            for particle in self.particles:\n",
        "                particle.update_velocity(self.gbest)\n",
        "                particle.update_position(self.bounds, self.dim)\n",
        "            break\n",
        "        return self.gbest, self.gbest_cost"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dp9tTlpItQhA"
      },
      "outputs": [],
      "source": [
        "\"\"\"def cost_function(position):\n",
        "  \n",
        "    # Define the LSTM-RNN model\n",
        "    model = Sequential()\n",
        "    model.add(LSTM(position[0], input_shape=(X_train.shape[1],1), return_sequences=True))\n",
        "    model.add(LSTM(position[1]))\n",
        "    model.add(Dense(y_train.shape[1], activation='linear'))\n",
        "    \n",
        "    model.compile(optimizer=SGD(learning_rate=0.01, decay=1e-6, momentum=0.9, nesterov=False),\n",
        "                  loss='mean_squared_error', metrics=['mae'])\n",
        "\n",
        "    # Fit the model to the training setX_train1\n",
        "    history = model.fit(X_train1, y_train1, epochs=5, batch_size=32, validation_data=(X_test, y_test))\n",
        "\n",
        "    # Make predictions on test data\n",
        "    y_pred_train1 = model.predict(X_train1)\n",
        "    y_pred_test1 = model.predict(X_test1)\n",
        "\n",
        "    # Evaluate model performance on training set\n",
        "    r21_train1 = r2_score(y_train1, y_pred_train1)\n",
        "    print('R2 score on training set: {:.2f}'.format(r21_train1))\n",
        "    mse_train1 = mean_squared_error(y_train1, y_pred_train1)\n",
        "    print(f\"Mean squared error on the training set: {mse_train1:.3f}\")\n",
        "    map_train1 = mean_absolute_error(y_train1, y_pred_train1)\n",
        "    print(f'Mean Absolute Percentage Error on training set:{map_train1:.3f}')\n",
        "    rmse_train1 = np.sqrt(mse_train1)\n",
        "    print(f\"Root Mean squared error on the training set: {rmse_train1:.3f}\")\n",
        "\n",
        "    # Evaluate model performance on test set\n",
        "    r21_test1 = r2_score(y_test1, y_pred_test1)\n",
        "    print('R2 score on test set: {:.2f}'.format(r21_test1))\n",
        "    mse_test1 = mean_squared_error(y_test1, y_pred_test1)\n",
        "    print(f\"Mean squared error on the test set: {mse_test1:.3f}\")\n",
        "    map_test1 = mean_absolute_error(y_test1, y_pred_test1)\n",
        "    print(f'Mean Absolute Percentage Error on test set:{map_test1:.3f}')\n",
        "    rmse_test1 = np.sqrt(mse_test1)\n",
        "    print(f\"Root Mean squared error on the test set: {rmse_test1:.3f}\")\n",
        "\n",
        "    val_loss = min(history.history['val_loss'])\n",
        "    return val_loss \n",
        "\n",
        "# Define the initial position, bounds, and other parameters for PSO\n",
        "x0 = [16,16]  # Initial hidden size for both GRU layers\n",
        "bounds = [ (8, 32), (8, 32)] #  constraints on hidden size\n",
        "num_particles = 10 # number of particles\n",
        "maxiter = 20\n",
        "hidden_size = 64\n",
        "\n",
        "# Create an instance of the PSO class and run the optimization\n",
        "pso = PSO(cost_function, x0, bounds, num_particles, maxiter)\n",
        "optimal_hidden_sizes, optimal_val_loss = pso.run()\n",
        "print(\"Optimal Hidden Sizes: \", optimal_hidden_sizes)\n",
        "print(\"Optimal Validation Loss: \", optimal_val_loss)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Si74Gv21mMN3"
      },
      "source": [
        "**DATSET 3**"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import random\n",
        "from sklearn.model_selection import train_test_split\n",
        "from keras.models import Sequential\n",
        "from keras.layers import GRU\n",
        "from keras.optimizers import Adam\n",
        "from keras.callbacks import EarlyStopping\n",
        "\n",
        "class Particle:\n",
        "    def __init__(self, x0):\n",
        "        self.position = x0\n",
        "        self.velocity = np.array([np.random.uniform(-1,1) for i in range(len(x0))])\n",
        "        self.best_pos_in = x0\n",
        "        self.best_pos_in\n",
        "        self.best_cost_in = float(\"-inf\")\n",
        "        \n",
        "\n",
        "    def update_velocity(self, global_best, c1=2, c2=2, w=0.7):\n",
        "        r1 = np.random.rand(len(self.velocity))\n",
        "        r2 = np.random.rand(len(self.velocity))\n",
        "\n",
        "        cognitive_tip = np.array(self.best_pos_in) - np.array(self.position)\n",
        "        cognitive_component = c1 * r1 * cognitive_tip\n",
        "\n",
        "        social_component = c2*r2*(global_best - self.position)\n",
        "        self.velocity = w*self.velocity + cognitive_component + social_component\n",
        "\n",
        "    def update_position(self, bounds, dim):\n",
        "        self.position += self.velocity\n",
        "        for i in range(dim):\n",
        "            if self.position[i] > bounds[i][1]:\n",
        "                self.position[i] = bounds[i][1]\n",
        "            elif self.position[i] < bounds[i][0]:\n",
        "                self.position[i] = bounds[i][0]\n",
        "\n",
        "class PSO:\n",
        "    def __init__(self, costFunc, x0, bounds, num_particles, maxiter):\n",
        "        self.costFunc = costFunc\n",
        "        self.num_particles = num_particles\n",
        "        self.dim = len(x0)\n",
        "        self.bounds = bounds\n",
        "        self.particles = []\n",
        "        self.gbest = np.zeros(self.dim)\n",
        "        self.gbest_cost = float(\"-inf\")\n",
        "        self.maxiter = maxiter\n",
        "\n",
        "        for i in range(self.num_particles):\n",
        "            self.particles.append(Particle(x0))\n",
        "\n",
        "\n",
        "    def run(self):\n",
        "        for i in range(self.maxiter):\n",
        "            for particle in self.particles:  \n",
        "        \n",
        "                cost = self.costFunc(particle.position)\n",
        "                \n",
        "                  \n",
        "                if cost < particle.best_cost_in:\n",
        "                    particle.best_cost_in = cost\n",
        "\n",
        "                    particle.best_pos_in = particle.position\n",
        "\n",
        "                if cost < self.gbest_cost:\n",
        "                    self.gbest_cost = cost\n",
        "                    self.gbest = particle.position\n",
        "\n",
        "            for particle in self.particles:\n",
        "                particle.update_velocity(self.gbest)\n",
        "                particle.update_position(self.bounds, self.dim)\n",
        "            break\n",
        "        return self.gbest, self.gbest_cost"
      ],
      "metadata": {
        "id": "kqx6IsE2XkWT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hNg2KEMJ-adf"
      },
      "outputs": [],
      "source": [
        "\"\"\"def cost_function(position):\n",
        "\n",
        "    # Define the LSTM-RNN model\n",
        "    model = Sequential()\n",
        "    model.add(LSTM(position[0], input_shape=(X_train.shape[1],1), return_sequences=True))\n",
        "    model.add(LSTM(position[1]))\n",
        "    model.add(Dense(y_train.shape[1], activation='linear'))\n",
        "\n",
        "    model.compile(optimizer=SGD(learning_rate=0.01, decay=1e-6, momentum=0.9, nesterov=False),\n",
        "                  loss='mean_squared_error', metrics=['mae'])\n",
        "   \n",
        "\n",
        "    # Fit the model to the training set\n",
        "    history = model.fit(X_train2, y_train2, epochs=5, batch_size=32, validation_data=(X_test2, y_test2))\n",
        "\n",
        "    # Evaluate model performance on training set\n",
        "    y_pred_train2 = model.predict(X_train2)\n",
        "    r22_train2 = r2_score(y_train2, y_pred_train2)\n",
        "    print('R2 score on training set: {:.2f}'.format(r22_train2))\n",
        "    mse_train2 = mean_squared_error(y_train2, y_pred_train2)\n",
        "    print(f\"Mean squared error on the training set: {mse_train2:.3f}\")\n",
        "    map_train2 = mean_absolute_error(y_train2, y_pred_train2)\n",
        "    print(f'Mean Absolute Percentage Error on training set:{map_train2:.3f}')\n",
        "    rmse_train2 = np.sqrt(mse_train2)\n",
        "    print(f\"Root Mean squared error on the training set: {rmse_train2:.3f}\")\n",
        "\n",
        "    # Evaluate model performance on test set\n",
        "    y_pred_test2 = model.predict(X_test2)\n",
        "    r22_test2 = r2_score(y_test2, y_pred_test2)\n",
        "    print('R2 score on test set: {:.2f}'.format(r22_test2))\n",
        "    mse_test2 = mean_squared_error(y_test2, y_pred_test2)\n",
        "    print(f\"Mean squared error on the test set: {mse_test2:.3f}\")\n",
        "    map_test2 = mean_absolute_error(y_test2, y_pred_test2)\n",
        "    print(f'Mean Absolute Percentage Error on test set:{map_test2:.3f}')\n",
        "    rmse_test2 = np.sqrt(mse_test2)\n",
        "    print(f\"Root Mean squared error on the test set: {rmse_test2:.3f}\")\n",
        "\n",
        "    val_loss = min(history.history['val_loss'])\n",
        "    return val_loss \n",
        "\n",
        "# Define the initial position, bounds, and other parameters for PSO\n",
        "x0 = [16,16]  # Initial hidden size for both GRU layers\n",
        "bounds = [ (8, 32), (8, 32)] #  constraints on hidden size\n",
        "num_particles = 10 # number of particles\n",
        "maxiter = 20\n",
        "hidden_size = 64\n",
        "\n",
        "# Create an instance of the PSO class and run the optimization\n",
        "pso = PSO(cost_function, x0, bounds, num_particles, maxiter)\n",
        "optimal_hidden_sizes, optimal_val_loss = pso.run()\n",
        "print(\"Optimal Hidden Sizes: \", optimal_hidden_sizes)\n",
        "print(\"Optimal Validation Loss: \", optimal_val_loss)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Size of input data: \", x.shape)\n",
        "print(\"Size of training set: \", y_train.shape)\n"
      ],
      "metadata": {
        "id": "3hWKkmAIeXL4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wY9rQ1lImQK1"
      },
      "source": [
        "**DATSET 4**"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import random\n",
        "from sklearn.model_selection import train_test_split\n",
        "from keras.models import Sequential\n",
        "from keras.layers import GRU\n",
        "from keras.optimizers import Adam\n",
        "from keras.callbacks import EarlyStopping\n",
        "\n",
        "class Particle:\n",
        "    def __init__(self, x0):\n",
        "        self.position = x0\n",
        "        self.velocity = np.array([np.random.uniform(-1,1) for i in range(len(x0))])\n",
        "        self.best_pos_in = x0\n",
        "        self.best_pos_in\n",
        "        self.best_cost_in = float(\"-inf\")\n",
        "        \n",
        "\n",
        "    def update_velocity(self, global_best, c1=2, c2=2, w=0.7):\n",
        "        r1 = np.random.rand(len(self.velocity))\n",
        "        r2 = np.random.rand(len(self.velocity))\n",
        "\n",
        "        cognitive_tip = np.array(self.best_pos_in) - np.array(self.position)\n",
        "        cognitive_component = c1 * r1 * cognitive_tip\n",
        "\n",
        "        social_component = c2*r2*(global_best - self.position)\n",
        "        self.velocity = w*self.velocity + cognitive_component + social_component\n",
        "\n",
        "    def update_position(self, bounds, dim):\n",
        "        self.position += self.velocity\n",
        "        for i in range(dim):\n",
        "            if self.position[i] > bounds[i][1]:\n",
        "                self.position[i] = bounds[i][1]\n",
        "            elif self.position[i] < bounds[i][0]:\n",
        "                self.position[i] = bounds[i][0]\n",
        "\n",
        "class PSO:\n",
        "    def __init__(self, costFunc, x0, bounds, num_particles, maxiter):\n",
        "        self.costFunc = costFunc\n",
        "        self.num_particles = num_particles\n",
        "        self.dim = len(x0)\n",
        "        self.bounds = bounds\n",
        "        self.particles = []\n",
        "        self.gbest = np.zeros(self.dim)\n",
        "        self.gbest_cost = float(\"-inf\")\n",
        "        self.maxiter = maxiter\n",
        "\n",
        "        for i in range(self.num_particles):\n",
        "            self.particles.append(Particle(x0))\n",
        "\n",
        "\n",
        "    def run(self):\n",
        "        for i in range(self.maxiter):\n",
        "            for particle in self.particles:  \n",
        "        \n",
        "                cost = self.costFunc(particle.position)\n",
        "                \n",
        "                  \n",
        "                if cost < particle.best_cost_in:\n",
        "                    particle.best_cost_in = cost\n",
        "\n",
        "                    particle.best_pos_in = particle.position\n",
        "\n",
        "                if cost < self.gbest_cost:\n",
        "                    self.gbest_cost = cost\n",
        "                    self.gbest = particle.position\n",
        "\n",
        "            for particle in self.particles:\n",
        "                particle.update_velocity(self.gbest)\n",
        "                particle.update_position(self.bounds, self.dim)\n",
        "            break\n",
        "        return self.gbest, self.gbest_cost"
      ],
      "metadata": {
        "id": "aPQWWNLWXld_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        " def cost_function(position):\n",
        "\n",
        "    # Define the LSTM-RNN model\n",
        "    model = Sequential()\n",
        "    model.add(LSTM(position[0], input_shape=(X_train.shape[1],1), return_sequences=True))\n",
        "    model.add(LSTM(position[1]))\n",
        "    model.add(Dense(y_train.shape[1], activation='linear'))\n",
        "\n",
        "    model.compile(optimizer=SGD(learning_rate=0.01, decay=1e-6, momentum=0.9, nesterov=False),\n",
        "                  loss='mean_squared_error', metrics=['mae'])\n",
        "\n",
        "    # Fit the model to the training set\n",
        "    history = model.fit(X_train3, y_train3, epochs=5, batch_size=32, validation_data=(X_test3, y_test3))\n",
        "\n",
        "    # Evaluate model performance on training set\n",
        "    y_pred_train3 = model.predict(X_train3)\n",
        "    r23_train3 = r2_score(y_train3, y_pred_train3)\n",
        "    print('R2 score on training set: {:.2f}'.format(r23_train3))\n",
        "    mse_train3 = mean_squared_error(y_train3, y_pred_train3)\n",
        "    print(f\"Mean squared error on the training set: {mse_train3:.3f}\")\n",
        "    map_train3 = mean_absolute_error(y_train3, y_pred_train3)\n",
        "    print(f'Mean Absolute Percentage Error on training set:{map_train3:.3f}')\n",
        "    rmse_train3 = np.sqrt(mse_train3)\n",
        "    print(f\"Root Mean squared error on the training set: {rmse_train3:.3f}\")\n",
        "\n",
        "    # Evaluate model performance on test set\n",
        "    y_pred_test3 = model.predict(X_test3)\n",
        "    r23_test3 = r2_score(y_test3, y_pred_test3)\n",
        "    print('R2 score on test set: {:.2f}'.format(r23_test3))\n",
        "    mse_test3 = mean_squared_error(y_test3, y_pred_test3)\n",
        "    print(f\"Mean squared error on the test set: {mse_test3:.3f}\")\n",
        "    map_test3 = mean_absolute_error(y_test3, y_pred_test3)\n",
        "    print(f'Mean Absolute Percentage Error on test set:{map_test3:.3f}')\n",
        "    rmse_test3 = np.sqrt(mse_test3)\n",
        "    print(f\"Root Mean squared error on the test set: {rmse_test3:.3f}\")\n",
        "\n",
        "    val_loss = min(history.history['val_loss'])\n",
        "    return val_loss \n",
        "\n",
        "# Define the initial position, bounds, and other parameters for PSO\n",
        "x0 = [16,16]  # Initial hidden size for both GRU layers\n",
        "bounds = [ (8, 32), (8, 32)] #  constraints on hidden size\n",
        "num_particles = 10 # number of particles\n",
        "maxiter = 20\n",
        "hidden_size = 64\n",
        "\n",
        "# Create an instance of the PSO class and run the optimization\n",
        "pso = PSO(cost_function, x0, bounds, num_particles, maxiter)\n",
        "optimal_hidden_sizes, optimal_val_loss = pso.run()\n",
        "print(\"Optimal Hidden Sizes: \", optimal_hidden_sizes)\n",
        "print(\"Optimal Validation Loss: \", optimal_val_loss)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 432
        },
        "id": "h83aTyqEF2xf",
        "outputId": "e3f3ee18-0a18-4592-c096-f2a81b5bd8d9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/5\n",
            "56634/56637 [============================>.] - ETA: 0s - loss: nan - mae: nan"
          ]
        },
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-40-5f5978d0c3f3>\u001b[0m in \u001b[0;36m<cell line: 49>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     47\u001b[0m \u001b[0;31m# Create an instance of the PSO class and run the optimization\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     48\u001b[0m \u001b[0mpso\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mPSO\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcost_function\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbounds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_particles\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmaxiter\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 49\u001b[0;31m \u001b[0moptimal_hidden_sizes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimal_val_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpso\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     50\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Optimal Hidden Sizes: \"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimal_hidden_sizes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Optimal Validation Loss: \"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimal_val_loss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-30-2d75801e42f9>\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     53\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mparticle\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparticles\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 55\u001b[0;31m                 \u001b[0mcost\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcostFunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparticle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mposition\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     56\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     57\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-40-5f5978d0c3f3>\u001b[0m in \u001b[0;36mcost_function\u001b[0;34m(position)\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m     \u001b[0;31m# Fit the model to the training set\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m     \u001b[0mhistory\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m32\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidation_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_test3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_test3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m     \u001b[0;31m# Evaluate model performance on training set\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/keras/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     68\u001b[0m             \u001b[0;31m# To get the full stack trace, call:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m             \u001b[0;31m# `tf.debugging.disable_traceback_filtering()`\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 70\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfiltered_tb\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     71\u001b[0m         \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     72\u001b[0m             \u001b[0;32mdel\u001b[0m \u001b[0mfiltered_tb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/keras/engine/data_adapter.py\u001b[0m in \u001b[0;36mselect_data_adapter\u001b[0;34m(x, y)\u001b[0m\n\u001b[1;32m   1080\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0madapter_cls\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1081\u001b[0m         \u001b[0;31m# TODO(scottzhu): This should be a less implementation-specific error.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1082\u001b[0;31m         raise ValueError(\n\u001b[0m\u001b[1;32m   1083\u001b[0m             \"Failed to find data adapter that can handle input: {}, {}\".format(\n\u001b[1;32m   1084\u001b[0m                 \u001b[0m_type_name\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_type_name\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: Failed to find data adapter that can handle input: <class 'function'>, <class 'pandas.core.series.Series'>"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def cost_function(position, X_train, y_train, X_test, y_test):\n",
        "\n",
        "    # Define the LSTM-RNN model\n",
        "    model = Sequential()\n",
        "    model.add(LSTM(position[0], input_shape=(X_train.shape[1],1), return_sequences=True))\n",
        "    model.add(LSTM(position[1]))\n",
        "    model.add(Dense(y_train.shape[1], activation='linear'))\n",
        "\n",
        "    model.compile(optimizer=SGD(learning_rate=0.01, decay=1e-6, momentum=0.9, nesterov=False),\n",
        "                  loss='mean_squared_error', metrics=['mae'])\n",
        "\n",
        "    # Fit the model to the training set\n",
        "    history = model.fit(X_train, y_train, epochs=5, batch_size=32, validation_data=(X_test, y_test))\n",
        "\n",
        "    # Evaluate model performance on training set\n",
        "    y_pred_train = model.predict(X_train)\n",
        "    r2_train = r2_score(y_train, y_pred_train)\n",
        "    print('R2 score on training set: {:.2f}'.format(r2_train))\n",
        "    mse_train = mean_squared_error(y_train, y_pred_train)\n",
        "    print(f\"Mean squared error on the training set: {mse_train:.3f}\")\n",
        "    map_train = mean_absolute_error(y_train, y_pred_train)\n",
        "    print(f'Mean Absolute Percentage Error on training set:{map_train:.3f}')\n",
        "    rmse_train = np.sqrt(mse_train)\n",
        "    print(f\"Root Mean squared error on the training set: {rmse_train:.3f}\")\n",
        "\n",
        "    # Evaluate model performance on test set\n",
        "    y_pred_test = model.predict(X_test)\n",
        "    r2_test = r2_score(y_test, y_pred_test)\n",
        "    print('R2 score on test set: {:.2f}'.format(r2_test))\n",
        "    mse_test = mean_squared_error(y_test, y_pred_test)\n",
        "    print(f\"Mean squared error on the test set: {mse_test:.3f}\")\n",
        "    map_test = mean_absolute_error(y_test, y_pred_test)\n",
        "    print(f'Mean Absolute Percentage Error on test set:{map_test:.3f}')\n",
        "    rmse_test = np.sqrt(mse_test)\n",
        "    print(f\"Root Mean squared error on the test set: {rmse_test:.3f}\")\n",
        "\n",
        "    val_loss = min(history.history['val_loss'])\n",
        "    return val_loss \n"
      ],
      "metadata": {
        "id": "siY2YawsSVcy"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}